{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Zeeshan Ali (01-134212-197)\n",
        "# NLP Assignment#03\n"
      ],
      "metadata": {
        "id": "R4_2907BAZU5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Jaccard Similarity (For Two Documents)"
      ],
      "metadata": {
        "id": "mw3nCxA1Aj7O"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c1A6bOdCAYLa",
        "outputId": "e196a039-233b-485a-ef35-e978776de543"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Jaccard Similarity: 0.5\n"
          ]
        }
      ],
      "source": [
        "with open('doc1.txt', 'r') as f:\n",
        "    doc1 = f.read().lower()\n",
        "\n",
        "with open('doc2.txt', 'r') as f:\n",
        "    doc2 = f.read().lower()\n",
        "\n",
        "set1 = set(doc1.split())\n",
        "set2 = set(doc2.split())\n",
        "\n",
        "intersection = 0\n",
        "for word in set1:\n",
        "    if word in set2:\n",
        "        intersection += 1\n",
        "\n",
        "union = len(set1) + len(set2) - intersection\n",
        "\n",
        "jaccard = intersection / union if union != 0 else 0\n",
        "print(\"Jaccard Similarity:\", jaccard)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cosine Similarity (For Two Documents)"
      ],
      "metadata": {
        "id": "pp1KqnKyCOoT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "words1 = doc1.split()\n",
        "words2 = doc2.split()\n",
        "\n",
        "# Create a unique list of words from both documents\n",
        "all_words = []\n",
        "for word in words1:\n",
        "    if word not in all_words:\n",
        "        all_words.append(word)\n",
        "for word in words2:\n",
        "    if word not in all_words:\n",
        "        all_words.append(word)\n",
        "\n",
        "# Initialize word count vectors\n",
        "vector1 = [0] * len(all_words)\n",
        "vector2 = [0] * len(all_words)\n",
        "\n",
        "# Populate word count vectors\n",
        "for i in range(len(all_words)):\n",
        "    vector1[i] = words1.count(all_words[i])\n",
        "    vector2[i] = words2.count(all_words[i])\n",
        "\n",
        "dot_product = 0\n",
        "magnitude1 = 0\n",
        "magnitude2 = 0\n",
        "\n",
        "for i in range(len(vector1)):\n",
        "    dot_product += vector1[i] * vector2[i]\n",
        "    magnitude1 += vector1[i] ** 2\n",
        "    magnitude2 += vector2[i] ** 2\n",
        "\n",
        "magnitude1 = magnitude1 ** 0.5\n",
        "magnitude2 = magnitude2 ** 0.5\n",
        "\n",
        "cosine = dot_product / (magnitude1 * magnitude2) if magnitude1 != 0 and magnitude2 != 0 else 0\n",
        "print(\"Cosine Similarity:\", cosine)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uxEVb9VJCESx",
        "outputId": "8dab973e-ad62-4f5b-a4f6-48a484573283"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cosine Similarity: 0.6761234037828131\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Jaccard Similarity Extension (Multiple Documents)"
      ],
      "metadata": {
        "id": "Ctu9VCAzCUiW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "query = \"artificial intelligence\"\n",
        "\n",
        "with open('document1.txt', 'r') as f:\n",
        "    document1 = f.read().lower()\n",
        "\n",
        "with open('document2.txt', 'r') as f:\n",
        "    document2 = f.read().lower()\n",
        "\n",
        "with open('document3.txt', 'r') as f:\n",
        "    document3 = f.read().lower()\n",
        "\n",
        "with open('document4.txt', 'r') as f:\n",
        "    document4 = f.read().lower()\n",
        "\n",
        "query_tokens = set(query.lower().split())\n",
        "documents = [document1, document2, document3, document4]\n",
        "\n",
        "jaccard_scores = []\n",
        "for i, doc in enumerate(documents):\n",
        "    doc_tokens = set(doc.split())\n",
        "\n",
        "    intersection = len(query_tokens & doc_tokens)\n",
        "    union = len(query_tokens | doc_tokens)\n",
        "\n",
        "    jaccard = intersection / union if union != 0 else 0\n",
        "    jaccard_scores.append((i + 1, jaccard))\n",
        "\n",
        "df = pd.DataFrame(jaccard_scores, columns=['Document', 'Jaccard Similarity'])\n",
        "df_sorted = df.sort_values(by='Jaccard Similarity', ascending=False)\n",
        "\n",
        "print(df_sorted)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C3G-LGwICdGt",
        "outputId": "742b5982-94d5-4089-9036-057f42674c97"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Document  Jaccard Similarity\n",
            "0         1            0.035714\n",
            "3         4            0.035088\n",
            "1         2            0.032787\n",
            "2         3            0.019608\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cosine Similarity Extension (Multiple Documents)"
      ],
      "metadata": {
        "id": "BnuzUoTeCavb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"medical field\"\n",
        "\n",
        "with open('document1.txt', 'r') as f:\n",
        "    document1 = f.read().lower()\n",
        "\n",
        "with open('document2.txt', 'r') as f:\n",
        "    document2 = f.read().lower()\n",
        "\n",
        "with open('document3.txt', 'r') as f:\n",
        "    document3 = f.read().lower()\n",
        "\n",
        "with open('document4.txt', 'r') as f:\n",
        "    document4 = f.read().lower()\n",
        "\n",
        "query_tokens = query.lower().split()\n",
        "query_word_counts = {}\n",
        "for word in query_tokens:\n",
        "    if word in query_word_counts:\n",
        "        query_word_counts[word] += 1\n",
        "    else:\n",
        "        query_word_counts[word] = 1\n",
        "\n",
        "documents = [document1, document2, document3, document4]\n",
        "\n",
        "cosine_scores = []\n",
        "for i, doc in enumerate(documents):\n",
        "    doc_tokens = doc.split()\n",
        "    doc_word_counts = {}\n",
        "    for word in doc_tokens:\n",
        "        if word in doc_word_counts:\n",
        "            doc_word_counts[word] += 1\n",
        "        else:\n",
        "            doc_word_counts[word] = 1\n",
        "\n",
        "    # Union of words for query and document vectors\n",
        "    all_words = list(set(query_tokens) | set(doc_tokens))\n",
        "\n",
        "    # Create vectors for query and document\n",
        "    query_vector = [query_word_counts.get(word, 0) for word in all_words]\n",
        "    doc_vector = [doc_word_counts.get(word, 0) for word in all_words]\n",
        "\n",
        "    # Calculate dot product and magnitudes\n",
        "    dot_product = sum(q * d for q, d in zip(query_vector, doc_vector))\n",
        "    magnitude_query = sum(q ** 2 for q in query_vector) ** 0.5\n",
        "    magnitude_doc = sum(d ** 2 for d in doc_vector) ** 0.5\n",
        "\n",
        "    # Compute Cosine similarity\n",
        "    cosine = dot_product / (magnitude_query * magnitude_doc) if magnitude_query and magnitude_doc else 0\n",
        "    cosine_scores.append((i + 1, cosine))\n",
        "\n",
        "df = pd.DataFrame(cosine_scores, columns=['Document', 'Cosine Similarity'])\n",
        "df_sorted = df.sort_values(by='Cosine Similarity', ascending=False)\n",
        "\n",
        "print(df_sorted)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bzp92kxpEZ4I",
        "outputId": "bd4cab7f-3276-47e4-8940-5ad8665f8e17"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Document  Cosine Similarity\n",
            "3         4           0.208013\n",
            "2         3           0.071429\n",
            "0         1           0.000000\n",
            "1         2           0.000000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# What If Query Is Not Known?\n",
        "## In a scenario where no specific query is provided, clustering methods such as k-means can help organize the dataset into groups of similar documents. Each cluster represents a group of closely related documents based on their word content."
      ],
      "metadata": {
        "id": "2H7Cu2djEyq-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "with open('document1.txt', 'r') as f:\n",
        "    document1 = f.read()\n",
        "\n",
        "with open('document2.txt', 'r') as f:\n",
        "    document2 = f.read()\n",
        "\n",
        "with open('document3.txt', 'r') as f:\n",
        "    document3 = f.read()\n",
        "\n",
        "with open('document4.txt', 'r') as f:\n",
        "    document4 = f.read()\n",
        "\n",
        "documents = [document1, document2, document3, document4]\n",
        "\n",
        "# Step 1: Convert documents into TF-IDF vectors\n",
        "tfidf_vectorizer = TfidfVectorizer(stop_words='english')\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(documents)\n",
        "\n",
        "# Step 2: Apply KMeans Clustering\n",
        "num_clusters = 2\n",
        "kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
        "kmeans.fit(tfidf_matrix)\n",
        "\n",
        "for i, cluster in enumerate(kmeans.labels_):\n",
        "    print(f\"Document {i+1} is in Cluster {cluster}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A7qB3iUyFr5z",
        "outputId": "ce51bdf1-51d6-44c6-94c6-ea028206d005"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Document 1 is in Cluster 0\n",
            "Document 2 is in Cluster 0\n",
            "Document 3 is in Cluster 1\n",
            "Document 4 is in Cluster 1\n"
          ]
        }
      ]
    }
  ]
}